{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping List of 300 Jobs\n",
    "s1 = '&start='\n",
    "url_loop_str = ['']\n",
    "for i in np.arange(25,300,25):\n",
    "    url_loop_str.append(s1+str(i))\n",
    "\n",
    "soup = bs4.BeautifulSoup()\n",
    "\n",
    "for i in url_loop_str:\n",
    "\n",
    "    base_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "    keyword = \"Data%20Science\"\n",
    "    location = \"United%20States\"\n",
    "    refresh = \"true\"\n",
    "    \n",
    "    soup_list = []\n",
    "\n",
    "    url = f\"{base_url}?keywords={keyword}&location={location}&refresh={refresh}{i}\"\n",
    "    page = requests.get(url)\n",
    "\n",
    "    recepie = bs4.BeautifulSoup(page.text,\"html\")\n",
    "\n",
    "    soup.append(recepie) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(soup):\n",
    "\n",
    "    role = []\n",
    "    c_name = []\n",
    "    dt = []\n",
    "    loc = []\n",
    "    j_url = []\n",
    "    c_url = []\n",
    "    ind = ''\n",
    "    \n",
    "    n_applicants = []\n",
    "    salary_range = []\n",
    "\n",
    "    seniority_level = []\n",
    "    employement_type = []\n",
    "    job_function = []\n",
    "    industry = []\n",
    "\n",
    "    about_info = []\n",
    "\n",
    "    raw_list = soup.find_all('li')\n",
    "\n",
    "    for i in range(len(raw_list)):\n",
    "        \n",
    "        # Position --------------------------------------------\n",
    "        role.append(raw_list[i].h3.contents[0].strip())\n",
    "        \n",
    "        # Company Name\n",
    "        if raw_list[i].h4.a:\n",
    "            c_name.append(raw_list[i].h4.a.contents[0].strip())\n",
    "        else: \n",
    "            c_name.append(raw_list[i].h4.contents[0].strip())\n",
    "        \n",
    "        # Job Location ----------------------------------------\n",
    "        if raw_list[i].find(class_ = 'job-search-card__location').contents[0].strip():\n",
    "            loc.append(raw_list[i].find(class_ = 'job-search-card__location').contents[0].strip())\n",
    "        else: \n",
    "            loc.append('')\n",
    "        \n",
    "        # Job Posting Date ------------------------------------\n",
    "        if raw_list[i].find(class_ = 'job-search-card__listdate'):\n",
    "            dt.append(raw_list[i].find(class_ = 'job-search-card__listdate').contents[0].strip())\n",
    "        else: \n",
    "            dt.append(raw_list[i].find(class_ = 'job-search-card__listdate--new').contents[0].strip())\n",
    "\n",
    "        # Job Post URL ----------------------------------------\n",
    "        if raw_list[i].find_all('a', href=True)[0]:\n",
    "            j_url.append(raw_list[i].find_all('a', href=True)[0]['href'])\n",
    "        else: \n",
    "            j_url.append('') \n",
    "        \n",
    "        if raw_list[i].find_all('a', href=True)[0]:\n",
    "            n_applicants_var, salary_range_var, seniority_level_var, employement_type_var, job_function_var, industry_var, about_info_var = extract_job_attr(raw_list[i].find_all('a', href=True)[0]['href'])\n",
    "        else:\n",
    "            n_applicants_var, salary_range_var, seniority_level_var, employement_type_var, job_function_var, industry_var, about_info_var = '', '', '', '', '', '', ''\n",
    "        \n",
    "        n_applicants.append(n_applicants_var)\n",
    "        salary_range.append(salary_range_var)\n",
    "\n",
    "        seniority_level.append(seniority_level_var)\n",
    "        employement_type.append(employement_type_var)\n",
    "        job_function.append(job_function_var)\n",
    "        industry.append(industry_var)\n",
    "\n",
    "        about_info.append(about_info_var)\n",
    "\n",
    "        # Company URL ----------------------------------------\n",
    "        if len(raw_list[i].find_all('a', href=True)) == 2:\n",
    "            c_url.append(raw_list[i].find_all('a', href=True)[1]['href'])\n",
    "        else: \n",
    "            c_url.append('')\n",
    "\n",
    "    df = pd.DataFrame(list(zip(role, c_name, loc, dt, j_url, c_url, \n",
    "                               n_applicants, salary_range, \n",
    "                               seniority_level, employement_type, job_function, industry, \n",
    "                               about_info)),\n",
    "                    columns =['JOB_ROLE', 'COMPANY', 'LOCATION', 'POST_DATE', 'JOB_URL', 'COMPANY_URL',\n",
    "                              'N_APPLICANTS', 'SALARY_RANGE', \n",
    "                              'SENIORITY_LEVEL', 'EMPLOYEMENT_TYPE', 'JOB_FUNCTION', 'INDUSTRY',\n",
    "                              'ABOUT'])\n",
    "\n",
    "    df = df[~df['JOB_ROLE'].str.upper().str.contains('INTERN')]\n",
    "\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Job Attributes\n",
    "def extract_job_attr(url):\n",
    "    if url == '':\n",
    "        return '', '', '', '', '', '', ''\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    n_applicants = ''\n",
    "    salary_range = ''\n",
    "\n",
    "    seniority_level = ''\n",
    "    employement_type = ''\n",
    "    job_function = ''\n",
    "    industry = ''\n",
    "    about_info = ''\n",
    "\n",
    "    # No. Applicants ----------------------------------------\n",
    "    if soup.find(class_ = 'num-applicants__caption'):\n",
    "        n_applicants = soup.find(class_ = 'num-applicants__caption').contents[0].strip()\n",
    "    else: \n",
    "        n_applicants = ''\n",
    "        \n",
    "    # Compensation ----------------------------------------\n",
    "    if soup.find(class_ = 'salary compensation__salary'):\n",
    "        salary_range = soup.find(class_ = 'salary compensation__salary').contents[0].strip()\n",
    "    else: \n",
    "        salary_range = ''\n",
    "\n",
    "    # Job Attributes --------------------------------------\n",
    "    job_attr = soup.find_all(class_ = 'description__job-criteria-text description__job-criteria-text--criteria')\n",
    "    if job_attr:\n",
    "        seniority_level = job_attr[0].contents[0].strip() if job_attr[0].contents else ''\n",
    "        employement_type = job_attr[1].contents[0].strip() if job_attr[1].contents else ''\n",
    "        job_function = job_attr[2].contents[0].strip() if job_attr[2].contents else ''\n",
    "        industry = job_attr[3].contents[0].strip() if job_attr[3].contents else ''\n",
    "    else:\n",
    "        seniority_level = ''\n",
    "        employement_type = ''\n",
    "        job_function = ''\n",
    "        industry = ''\n",
    "    \n",
    "    # About ----------------------------------------------\n",
    "    if soup.find(class_ = 'show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden'):\n",
    "        for i in soup.find(class_ = 'show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden').contents:\n",
    "            about_info += str(i)\n",
    "    \n",
    "    return n_applicants, salary_range, seniority_level, employement_type, job_function, industry, about_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get(soup)\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(data):\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    s1 = ''\n",
    "\n",
    "    for i,j in data.ABOUT.iteritems():\n",
    "        s1 += j\n",
    "\n",
    "    s1 = s1.replace('<br/>',' ').replace('</strong>',' ').replace('<strong>', ' ').replace('</li>', ' ').replace('<li>', ' ').replace('<ul>', ' ').replace('</ul>', ' ').replace('<em>', ' ').replace('</em>', ' ').replace('</p>', ' ').replace('<p>', ' ').lower()\n",
    "    s2 = s1.split(' ') \n",
    "\n",
    "    s3 = [i for i in s2 if i not in stopwords.words('english')]\n",
    "\n",
    "    res = dict(Counter(s3))\n",
    "\n",
    "    res2 = {}\n",
    "    for w in sorted(res, key=res.get, reverse=True):\n",
    "        res2[w] = res[w]\n",
    "    return pd.DataFrame(res2.items())\n",
    "\n",
    "frequency_analysis = get_freq(data)\n",
    "frequency_analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('LinkedIn Job DS.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "data.to_excel(writer, sheet_name= 'Jobs', index = False)\n",
    "frequency_analysis.to_excel(writer, sheet_name= 'Frequency', index = False)\n",
    "\n",
    "writer.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4d68a23e7b17350b5bd8ece07a2edd2e738708fe9640219d0e8e39fd293cd5a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
